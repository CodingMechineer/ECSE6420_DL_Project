{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('A-YOLOM_pretrained_model/v4.pt')  # load a pretrained model (recommended for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConfig:\n",
    "    def __init__(self):\n",
    "        self.path = \"C:/Users/manue/OneDrive - University of Georgia/201 - Deep Learning - ECSE 6420/100 - Project/BDD100k\"\n",
    "        self.train = \"images/train2017\"\n",
    "        self.val = \"images/val2017\"\n",
    "        self.test = \"images/val2017\"\n",
    "        self.labels_list = [\"detection-object\", \"seg-drivable-10\", \"seg-lane-11\"]\n",
    "        self.tnc = 3\n",
    "        self.nc_list = [1,1,1]\n",
    "        self.map = [None,{'10':'0'},{'11':'0'}]\n",
    "        self.names = {\n",
    "            0: \"person\",\n",
    "            1: \"rider\",\n",
    "            2: \"car\",\n",
    "            3: \"bus\",\n",
    "            4: \"truck\",\n",
    "            5: \"bike\",\n",
    "            6: \"motor\",\n",
    "            7: \"traffic light\",\n",
    "            8: \"traffic sign\",\n",
    "            9: \"train\",\n",
    "            10: \"drivable\",\n",
    "            11: \"lane\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabc30fc-e7726578.jpg: 416x736 3 persons, 308.2ms\n",
      "image 2/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabc30fc-eb673c5a.jpg: 416x736 9 persons, 214.9ms\n",
      "image 3/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabc30fc-fd79926f.jpg: 416x736 5 persons, 213.4ms\n",
      "image 4/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabc9045-1b8282ba.jpg: 416x736 20 persons, 217.4ms\n",
      "image 5/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabc9045-581f64de.jpg: 416x736 6 persons, 210.4ms\n",
      "image 6/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabc9045-5a50690f.jpg: 416x736 3 persons, 210.4ms\n",
      "image 7/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabc9045-b3349548.jpg: 416x736 3 persons, 212.4ms\n",
      "image 8/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabc9045-c6dc9529.jpg: 416x736 11 persons, 213.5ms\n",
      "image 9/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabc9045-cd422b81.jpg: 416x736 14 persons, 212.4ms\n",
      "image 10/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabc9045-d91ecb66.jpg: 416x736 18 persons, 211.4ms\n",
      "image 11/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabddb96-ca0ac856.jpg: 416x736 6 persons, 212.4ms\n",
      "image 12/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabe1040-5f02711e.jpg: 416x736 4 persons, 241.4ms\n",
      "image 13/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabe1040-c59cb390.jpg: 416x736 6 persons, 212.4ms\n",
      "image 14/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabea010-6882cf41.jpg: 416x736 9 persons, 211.4ms\n",
      "image 15/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabf7be1-36a39a28.jpg: 416x736 11 persons, 213.4ms\n",
      "image 16/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabf7be1-f1a7e00d.jpg: 416x736 19 persons, 211.5ms\n",
      "image 17/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cabf9f3c-d58a6760.jpg: 416x736 10 persons, 214.4ms\n",
      "image 18/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac07407-0396e053.jpg: 416x736 5 persons, 214.4ms\n",
      "image 19/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac07407-0eb1c8bf.jpg: 416x736 10 persons, 212.4ms\n",
      "image 20/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac07407-15b814db.jpg: 416x736 13 persons, 219.4ms\n",
      "image 21/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac07407-196cd6f8.jpg: 416x736 9 persons, 216.4ms\n",
      "image 22/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac07407-76e4c968.jpg: 416x736 21 persons, 222.9ms\n",
      "image 23/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac07407-951977c8.jpg: 416x736 7 persons, 201.5ms\n",
      "image 24/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac07407-ba37148a.jpg: 416x736 11 persons, 128.7ms\n",
      "image 25/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac07407-bc0b048a.jpg: 416x736 17 persons, 119.6ms\n",
      "image 26/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac07407-e969f06a.jpg: 416x736 20 persons, 118.7ms\n",
      "image 27/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac07407-fe32e494.jpg: 416x736 7 persons, 116.7ms\n",
      "image 28/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac32276-2ea56b83.jpg: 416x736 10 persons, 120.7ms\n",
      "image 29/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac32276-a70feba7.jpg: 416x736 17 persons, 117.7ms\n",
      "image 30/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac32276-cf233a28.jpg: 416x736 10 persons, 118.7ms\n",
      "image 31/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac47e88-3227e13a.jpg: 416x736 13 persons, 116.7ms\n",
      "image 32/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac6a8d8-29780d2d.jpg: 416x736 2 persons, 116.9ms\n",
      "image 33/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cac75967-8a9c164b.jpg: 416x736 2 persons, 117.7ms\n",
      "image 34/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cace2484-cd478fab.jpg: 416x736 4 persons, 115.7ms\n",
      "image 35/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cacfaddc-ce8105be.jpg: 416x736 20 persons, 118.7ms\n",
      "image 36/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad02f4a-dd2c4b41.jpg: 416x736 14 persons, 117.7ms\n",
      "image 37/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad03998-075bb24b.jpg: 416x736 20 persons, 218.4ms\n",
      "image 38/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad03998-b2cc3e61.jpg: 416x736 14 persons, 215.9ms\n",
      "image 39/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad03998-e6cc2ae6.jpg: 416x736 13 persons, 217.4ms\n",
      "image 40/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad17389-cb0208f5.jpg: 416x736 2 persons, 212.4ms\n",
      "image 41/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad17389-f72e5365.jpg: 416x736 2 persons, 216.4ms\n",
      "image 42/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad180c4-2edc9315.jpg: 416x736 11 persons, 213.4ms\n",
      "image 43/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad180c4-553ceeb1.jpg: 416x736 5 persons, 216.4ms\n",
      "image 44/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad180c4-d05b49f1.jpg: 416x736 3 persons, 216.9ms\n",
      "image 45/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad4a646-68a8d136.jpg: 416x736 16 persons, 219.4ms\n",
      "image 46/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad54cf7-6779254f.jpg: 416x736 4 persons, 130.2ms\n",
      "image 47/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cad7fdff-d9946f73.jpg: 416x736 9 persons, 126.7ms\n",
      "image 48/48 C:\\Users\\manue\\OneDrive - University of Georgia\\201 - Deep Learning - ECSE 6420\\100 - Project\\BDD100k\\images\\example\\cadb5cf7-31d9a0d7.jpg: 416x736 12 persons, 127.6ms\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/manue/OneDrive - University of Georgia/201 - Deep Learning - ECSE 6420/100 - Project/BDD100k/images/example\"\n",
    "\n",
    "# Run the model prediction\n",
    "predictions = model.predict(source=path, imgsz=(1280,736), device=1, name='First_test', save=True, conf=0.25, iou=0.45, show_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.105  Python-3.10.13 torch-2.2.0+cu118 CUDA:0 (NVIDIA GeForce 940MX, 2048MiB)\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=multi, mode=train, model=A-YOLOM_pretrained_model/v4.pt, data=bdd-multi.yaml, epochs=1, patience=50, batch=12, imgsz=(1280, 736), save=True, save_period=-1, cache=False, device=[0], workers=8, project=None, name=Testrun, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=True, combine_class=None, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, overlap_mask=True, mask_ratio=1, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, speed=False, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=[2, 3, 4], retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, TL=8.0, FL=24.0, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, binary_mask_threshold=0.5, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=c:\\users\\manue\\onedrive - university of georgia\\201 - deep learning - ecse 6420\\100 - project\\yolov8-multi-task\\runs\\multi\\Testrun4\n",
      "WARNING  no model scale passed. Assuming scale='n'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22                   9  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 23             [-1, 6]  1     98561  ultralytics.nn.modules.conv.Concat_dropout   [1]                           \n",
      " 24                  -1  1    131840  ultralytics.nn.modules.block.C2f             [256, 128, 1]                 \n",
      " 25                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 26             [-1, 4]  1     24705  ultralytics.nn.modules.conv.Concat_dropout   [1]                           \n",
      " 27                  -1  1     33152  ultralytics.nn.modules.block.C2f             [128, 64, 1]                  \n",
      " 28                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 29             [-1, 2]  1      6209  ultralytics.nn.modules.conv.Concat_dropout   [1]                           \n",
      " 30                  -1  1      8384  ultralytics.nn.modules.block.C2f             [64, 32, 1]                   \n",
      " 31                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 32             [-1, 0]  1      1569  ultralytics.nn.modules.conv.Concat_dropout   [1]                           \n",
      " 33                  -1  1      2144  ultralytics.nn.modules.block.C2f             [32, 16, 1]                   \n",
      " 34                   9  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 35             [-1, 6]  1     98561  ultralytics.nn.modules.conv.Concat_dropout   [1]                           \n",
      " 36                  -1  1    131840  ultralytics.nn.modules.block.C2f             [256, 128, 1]                 \n",
      " 37                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 38             [-1, 4]  1     24705  ultralytics.nn.modules.conv.Concat_dropout   [1]                           \n",
      " 39                  -1  1     33152  ultralytics.nn.modules.block.C2f             [128, 64, 1]                  \n",
      " 40                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 41             [-1, 2]  1      6209  ultralytics.nn.modules.conv.Concat_dropout   [1]                           \n",
      " 42                  -1  1      8384  ultralytics.nn.modules.block.C2f             [64, 32, 1]                   \n",
      " 43                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 44             [-1, 0]  1      1569  ultralytics.nn.modules.conv.Concat_dropout   [1]                           \n",
      " 45                  -1  1      2144  ultralytics.nn.modules.block.C2f             [32, 16, 1]                   \n",
      " 46        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      " 47                [45]  1      7940  ultralytics.nn.modules.head.Segment          [1, 32, 64, [16]]             \n",
      " 48                [33]  1      7940  ultralytics.nn.modules.head.Segment          [1, 32, 64, [16]]             \n",
      "YOLOv8-bdd-v4-one-dropout-individual summary: 397 layers, 3640051 parameters, 3640003 gradients\n",
      "\n",
      "Transferred 613/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "WARNING  updating to 'imgsz=1280'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 95 weight(decay=0.0), 122 weight(decay=0.00046875), 111 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\bdd100k\\detection-object\\labels\\train.cache... 70000 images, 0 backgrounds, 0 corrupt: 100%|██████████| 70000/70000 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\bdd100k\\images\\train\\75055858-7d04a650.jpg: 1 duplicate labels removed\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, \"YOLOv8-multi-task/ultralytics\")\n",
    "# # 现在就可以导入Yolo类了\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO('yolov8-bdd-v4-one-dropout-individual-s.yaml', task='multi')  # build a new model from YAML\n",
    "\n",
    "model = YOLO('A-YOLOM_pretrained_model/v4.pt')  # load a pretrained model (recommended for training)\n",
    "# model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n",
    "\n",
    "# Train the model\n",
    "# model.train(data='/home/jiayuan/yolom/ultralytics/datasets/bdd-multi.yaml', batch=12, epochs=300, imgsz=(640,640), device=[0,1,2], name='yolopm', val=True, task='multi',classes=[2,3,4,9,10,11],combine_class=[2,3,4,9],single_cls=True)\n",
    "results = model.train(data='bdd-multi.yaml', batch=12, epochs=1, imgsz=(1280,736), device=[0], name='Testrun', val=True, task='multi', classes=[2,3,4], single_cls=True, plots=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
